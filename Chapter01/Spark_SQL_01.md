## 1.3 스파크 SQL 및 DataFrames를 사용한 빅 데이터 처리

##### 스파크 SQL 엔진은 DataFrame 및 스파크 SQL의 두 가지 유형의 API를 지원합니다. RDD보다 높은 수준의 추상화이므로 훨씬 더 직관적이고 표현력이 뛰어납니다. 데이터 엔지니어, 데이터 분석가 또는 데이터 과학자로서 이미 익숙할 수 있는 더 많은 데이터 변환 기능 및 유틸리티와 함께 제공됩니다.
##### 따라서, Spark SQL 및 DataFrame API는 빅 데이터 처리에 대한 진입 장벽을 낮춥니다. 이를 통해 데이터 분석에 대한 기존 지식과 기술을 사용할 수 있으며 분산 컴퓨팅을 쉽게 시작할 수 있습니다. 분산 컴퓨팅 프레임워크와 함께 일반적으로 발생하는 복잡성을 처리할 필요 없이 대규모 데이터 처리를 시작할 수 있도록 도와줍니다. 이 절에서는 DataFrame 및 Spark SQL API를 모두 사용하여 확장 가능한 데이터 처리 여정을 시작하는 방법에 대해 설명합니다. 

  - [1.3.1 Spark DataFrame으로 데이터 변환](https://github.com/synabreu/Spark/blob/main/Chapter01/Spark_SQL_02.md)
  - [1.3.2 Spark 상에서 SQL 사용하기](https://github.com/synabreu/Spark/blob/main/Chapter01/Spark_SQL_03.md) 
  - [1.3.3 아파치 스파크 3.0 새 기능 소개](https://github.com/synabreu/Spark/blob/main/Chapter01/Spark_SQL_04.md)


##### 전체적으로 요약하자면, 생성되는 데이터의 양이 빠르게 증가하고 단일 전문 시스템을 사용하여 모든 데이터를 처리하는 것이 실용적이지 않거나 실현 가능하지 않기 때문에 분산 컴퓨팅이 매우 중요한 이유를 배웠습니다. 그런 다음 데이터 병렬 처리의 개념에 대해 배우고 MapReduce 패러다임을 통한 구현의 실제 예를 검토했습니다. 그런 다음 아파치 스파크라는 인메모리 통합 분석 엔진을 소개하고 데이터 처리에 얼마나 빠르고 효율적인지 배웠습니다. 또한 데이터 처리 응용 프로그램 개발을 시작하는 것이 매우 직관적이고 쉽다는 것과 아파치 스파크의 아키텍처와 구성 요소와 이들이 어떻게 하나의 프레임워크로 결합되는지 이해했습니다. 

##### 다음으로 Apache Spark의 핵심 추상화인 RDD, 분산 방식으로 머신 클러스터에 데이터를 저장하는 방법, 람다 함수와 함께 고차 함수를 활용하여 데이터 병렬 처리를 구현하는 방법을 이해하게 되었습니다. 또한 Apache Spark의 Spark SQL 엔진 구성 요소, RRD보다 높은 수준의 추상화를 제공하는 방법, 이미 익숙할 수 있는 몇 가지 기본 제공 기능에 대해 배웠습니다. DataFrame DSL을 활용하여 보다 쉽고 친숙한 방식으로 데이터 처리 비즈니스 로직을 구현하는 방법을 배웠습니다. 또한 Spark의 SQL API, ANSI SQL 표준 준수 및 대량 데이터에 대한 SQL 분석을 효율적으로 원활하게 수행하는 방법에 대해 배웠습니다.

##### 또한 적응 쿼리 실행 및 동적 파티션 정리와 같은 Apache Spark 3.0의 눈에 띄는 개선 사항 중 일부를 알게 되었으며, 이를 통해 Spark 3.0의 성능이 이전 버전보다 훨씬 빨라졌습니다. Apache Spark를 사용한 빅 데이터 처리의 기본 사항을 배웠으므로 이제 Spark를 사용하여 데이터 분석 여정을 시작할 준비가 되었습니다. 일반적인 데이터 분석 여정은 다양한 소스 시스템에서 원시 데이터를 수집하여 데이터 웨어하우스 또는 데이터 레이크와 같은 레코드 스토리지 컴포넌트로 수집한 다음 단일 소스를 얻기 위해 정제, 통합 및 변환하여 원시 데이터를 변환하는 것으로 시작됩니다. 마지막으로, 기술 및 예측 분석을 활용하여 깨끗하고 통합된 데이터를 통해 실행 가능한 비즈니스 통찰력을 얻을 수 있습니다.
