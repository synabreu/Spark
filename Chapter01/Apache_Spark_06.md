### 1.2.5 스파크 시작하기

##### 지금까지 RDD라고 하는 아파치 스파크의 핵심 데이터 구조, RDD를 조작하는 데 사용되는 고차 함수라고 하는 함수 및 아파치 스파크 클러스터의 구성 요소에 대해 배웠습니다. 고차 함수를 사용하는 방법에 대한 몇 가지 코드 조각도 보았습니다. 그렇다면, 이 절에서는 지식을 실제 사용에 적용하고 PySpark라는 스파크의 Python API를 사용하여 단어 수 계산 애플리케이션을 만드는 첫 번째 아파치 스파크 프로그램을 작성합니다. 그러나 먼저 시작하려면 몇 가지 사항이 필요합니다.

  - ##### 아파치 스파크 클러스터
  - ##### 데이터세트
  - ##### 단어 수 응용 프로그램의 실제 코드

##### 스파크 클러스터를 생성하기 위해 Databricks의 무료 Community Edition을 사용할 것입니다. 사용된 코드는 여기에 있고, 필요한 리소스에 대한 링크는 장의 시작 부분에 있는 기술 요구 사항 섹션에서 찾을 수 있습니다.

***알림***

##### 여기에서는 Databricks Spark Clusters를 사용하고 있지만 Spark 클러스터에서 액세스할 수 있는 위치에서 데이터가 제공되는 한 스파크 3.0 이상을 실행하는 모든 Spark 클러스터에서 제공된 코드를 실행할 수 있습니다. 이제 RDD, 고차 함수, 람다 및 Spark 아키텍처와 같은 Spark의 핵심 개념을 이해했으므로 다음 코드를 사용하여 첫 번째 Spark 애플리케이션을 구현해 보겠습니다.

~~~
lines = sc.textFile("/databricks-datasets/README.md") 
words = lines.flatMap(lambda s: s.split(" ")) 
word_tuples = words.map(lambda s: (s, 1)) 
word_count = word_tuples.reduceByKey(lambda x, y:  x + y) 
word_count.take(10) 
word_count.saveAsTextFile("/tmp/wordcount.txt") 
~~~

##### 위의 소스 코드 조각 모음의 동작 설명은 다음과 같습니다. 

-- ##### 1. 내장된 sc.textFile() 메서드를 사용하여 텍스트 파일을 로드합니다. 이 메서드는 지정된 위치의 모든 텍스트 파일을 읽고 개별 줄로 분할하고 줄 또는 문자열의 RDD를 반환합니다.
-- ##### 2. 그런 다음 flatMap() 고차 함수를 라인의 RDD에 적용하고 각 라인을 공백을 기준으로 분할하도록 지시하는 함수를 제공합니다. 우리가 flatMap()에 전달하는 람다 함수는 단순히 하나의 파라미터인 행을 취하고 개별 단어를 목록으로 반환하는 익명의 함수(anonymous function)입니다. flatMap() 및 lambda() 함수를 사용하여 행의 RDD를 단어의 RDD로 변환할 수 있습니다.
-- ##### 3. 그런 다음 map() 함수를 사용하여 모든 개별 단어에 1의 개수를 할당합니다.
-- ##### 4. 마지막으로, 여러 번 발생하는 유사한 단어의 수를 합산하기 위해 reduceByKey() 고차 함수를 사용합니다.
-- ##### 5. 개수가 계산되면 take() 함수를 사용하여 최종 단어 개수의 표본을 표시합니다.
-- ##### 6. 표본 결과 집합을 표시하는 것이 일반적으로 코드의 정확성을 결정하는 데 도움이 되지만, 빅 데이터 설정에서 모든 결과를 콘솔에 표시하는 것은 실용적이지 않습니다.따라서 saveAsTextFile() 함수를 사용하여 최종 결과를 영구 저장소에 유지합니다.

***중요 사항***
##### take() 또는 collect()와 같은 명령을 사용하여 전체 결과 세트를 콘솔에 표시하지 않는 것이 좋습니다. 빅 데이터 설정에서 모든 데이터를 시도하고 표시하는 것은 완전히 위험할 수도 있습니다. 너무 많은 데이터를 드라이버로 다시 가져오려고 시도하고 드라이버가 OutOfMemoryError와 함께 실패하도록 할 수 있기 때문에 전체 응용 프로그램이 실패합니다. 따라서, 매우 작은 결과 집합으로 take()를 사용하고 반환되는 데이터 양이 실제로 매우 적다고 확신할 때만 collect()를 사용하는 것이 좋습니다.

##### 람다의 내부 동작과 고차 함수와 함께 데이터 병렬 처리를 구현하는 방법을 이해하기 위해 다음 코드 줄을 봐 주세요! 

~~~
words = lines.flatMap(lambda s: s.split(" "))
~~~

##### 위의 소스 코드 조각 모음에서 flatMmap() 고차 함수는 람다에 있는 코드를 번들하고 직렬화(Serialization)라는 프로세스를 사용하여 네트워크를 통해 워커 노드로 보냅니다. 그런 다음, 이 직렬화된 람다가 모든 executor로 전송되고 각 executor는 차례로 이 람다를 개별 RDD 파티션에 병렬로 적용합니다.

 
***중요 사항***
##### 고차 함수는 Executor에 코드를 보내기 위해 람다를 직렬화할 수 있어야 하기 때문에. 람다 함수는 직렬화 가능해야 하며, 실패하면 직렬화 불가능 태스크 오류가 발생할 수 있습니다.

##### 전체적으로 요약하면, 고차 함수는 본질적으로 직렬화된 람다 형식의 데이터 변환 코드를 RDD 파티션의 데이터로 전송하는 것입니다. 따라서 코드가 있는 위치로 데이터를 이동하는 대신 실제로 데이터가 있는 위치로 코드를 이동합니다. 이것이 바로 데이터 병렬 처리를 하는 것을 합니다.따라서, 아파치 스파크는 RDD 및 고차 함수와 함께 데이터 병렬 처리 패러다임의 메모리 내 버전을 구현합니다. 이를 통해 아파치 스파크는 분산 컴퓨팅 설정에서 빅 데이터 처리를 빠르고 효율적으로 수행할 수 있습니다.

##### 아파치 스파크의 RDD 추상화는 확실히 MapReduce에 비해 더 높은 수준의 프로그래밍 API를 제공하지만, 가장 일반적인 유형의 데이터 변환도 표현할 수 있으려면 함수를 사용하는 프로그래밍 스타일에 대한 어느 정도 수준의 이해가 필요합니다. 이 문제를 극복하기 위해 Spark의 기존 SQL 엔진이 확장되었고 DataFrame이라는 또 다른 추상화가 RDD 위에 추가되었습니다. 이는 데이터 과학자와 데이터 분석가에게 데이터 처리를 훨씬 쉽고 친숙하게 만듭니다. 
