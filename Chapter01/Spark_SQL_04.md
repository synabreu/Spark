#### 1.3.2 스파크에서 SQL 사용하기

##### SQL은 임시 데이터 탐색 및 비즈니스 인텔리전스(BI) 유형의 쿼리를 위한 표현 언어입니다. 매우 높은 수준의 선언적 프로그래밍 언어이기 때문에 사용자는 단순히 입력 및 출력과 데이터에 수행해야 하는 작업에 집중할 수 있으며, 실제로 논리를 구현하는 방법의 프로그래밍 복잡성에 대해 너무 걱정하지 않아도 됩니다. 아파치 스파크의 SQL 엔진에는 DataFrame 및 Dataset API와 함께 SQL 언어 API도 있습니다.

##### 스파크 3.0을 통해 스파크 SQL은 이제 ANSI 표준을 준수하므로 다른 SQL 기반 플랫폼에 익숙한 데이터 분석가라면 최소한의 노력으로 스파크 SQL을 시작할 수 있습니다.

##### DataFrames와 Spark SQL은 동일한 기본 Spark SQL 엔진을 사용하므로 완전히 상호 교환 가능하며 SQL로 쉽게 표현되는 코드 부분에 대해 사용자가 DataFrame DSL과 Spark SQL 문을 혼합하는 경우가 많습니다. 이제 Spark SQL을 사용하여 단어 수 계산 프로그램을 다시 작성해 보겠습니다. 먼저 텍스트 파일을 구분 기호로 공백이 있는 CSV 파일로 지정하는 테이블을 만듭니다. 텍스트 파일의 각 줄을 읽고 각 파일을 한 번에 개별 단어로 분할하는 깔끔한 로직입니다.

~~~
CREATE TABLE word_counts (word STRING) 
USING csv 
OPTIONS("delimiter"=" ") 
LOCATION "/databricks-datasets/README.md" 
~~~

##### 이제 단일 단어 열의 테이블이 있으므로 단어 열을 GROUP BY하고, 단어 수를 얻기 위해 COUNT() 작업을 수행하기만 하면 됩니다.

~~~
SELECT word, COUNT(word) AS count 
FROM word_counts 
GROUP BY word 
~~~

##### 여기에서 동일한 비즈니스 문제를 해결하는 것이 MapReduce를 사용하여 RRD, DataFrame 및 Spark SQL에 이르기까지 점점 더 쉬워졌다는 것을 알 수 있습니다. 새로운 릴리스마다 아파치 스파크는 더 높은 수준의 프로그래밍 추상화, 데이터 변환 및 유틸리티 기능, 기타 최적화를 추가했습니다. 그러한 목표는 데이터 엔지니어, 데이터 과학자 및 데이터 분석가가 복잡한 프로그래밍 추상화 또는 시스템 아키텍처에 대해 걱정하지 않고, 당면한 실제 비즈니스 문제를 해결하는 데 시간과 에너지를 집중할 수 있도록 하는 것이었습니다. 참고로 아파치 스파크의 최신 메이저 버전 3.0 에는 데이터 분석 전문가의 삶을 훨씬 더 쉽게 만들어주는 많은 개선 사항이 있습니다. 
